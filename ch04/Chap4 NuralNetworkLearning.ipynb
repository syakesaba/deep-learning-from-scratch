{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ駆動、人駆動"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データ駆動：人の意識なく集められたデータのみを信用する考え方  \n",
    "- 人駆動：人の意識によって集められた特徴量（ベクトル）、人の意識によって集められた分類法によって得られたデータを使用する考え方  \n",
    "- 教師データ：ニューラルネットワークでいう、学習データ。学習データはディープラーニングにおいてデータ駆動として扱う都合テストデータと一致してはならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 過学習(overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ディープラーニングのおいて特定個人・個別のデータの特徴量のみ意識してしまい汎化能力に欠けてしまうこと。\n",
    "ディープラーニングに置いては 学習データ≠テストデータ≠将来的なデータ でなければならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数(loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "出力における、テスト結果における正解との誤差のこと。平均二乗誤差(mean squared error)と交差エントロピー誤差（cross entropy error）の二つが代表的にある。この損失関数の値が小さいほど正解に近いということになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = 1/2(\\Sigma_k(y_k - t_k)^2 )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "24.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "a1,a2 = np.asarray([1,2,3]), np.asarray([1,2,4])\n",
    "print(mean_squared_error(a1,a2))\n",
    "a1,a2 = np.asarray([1,2,3]), np.asarray([1,2,10])\n",
    "print( mean_squared_error(a1,a2) )\n",
    "a1,a2 = np.asarray([1,2,3]), np.asarray([1,2,3])\n",
    "print( mean_squared_error(a1,a2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = -(\\Sigma_k(t_k \\log y_t)^2 )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log(x)の関数の特徴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1102937048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#plt.show()をinlineにて表示する\n",
    "#Esc+Lで行番号を表示する\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "def log(x):\n",
    "    return np.log(x)\n",
    "x = np.arange(0.01,100,0.01)\n",
    "y = log(x)\n",
    "\n",
    "plt.plot(x,y,label=\"log(x)\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.title(\"y=log(x)\")\n",
    "\n",
    "plt.legend()#show label box\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交差エントロピー誤差の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    return np.exp(array) / np.sum(np.exp(array))\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7 #Avoid log(0) => Inf\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM t = 1\n",
      "SUM y = 1.0\n",
      "交差エントロピー誤差 =  0.510825457099\n",
      "SUM t = 1\n",
      "SUM y = 1.0\n",
      "交差エントロピー誤差 =  2.30258409299\n"
     ]
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]#One-Shot配列（正解＝１、不正解＝０に正規化したもの）\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]#SoftMax関数により正規化済\n",
    "print(\"SUM t =\",np.sum(np.array(t)))\n",
    "print(\"SUM y =\", np.sum(np.array(y)))\n",
    "print(\"交差エントロピー誤差 = \",cross_entropy_error(np.array(y),np.array(t)))\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]#One-Shot配列（正解＝１、不正解＝０に正規化したもの）\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]#SoftMax関数により正規化済\n",
    "print(\"SUM t =\",np.sum(np.array(t)))\n",
    "print(\"SUM y =\", np.sum(np.array(y)))\n",
    "print(\"交差エントロピー誤差 = \",cross_entropy_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ミニバッチ学習（サンプリング）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数を行列横断的に実施するためには、以下の関数を用いる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$ E = -(1/N)\\Sigma_n\\Sigma_k(t_(nk)\\log y_(nk))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 関数は難しいが単純に損失関数を各配列ごとにやって最後に足しているだけ。それぞれの配列の内容や要素数はもちろん異なるため、それぞれ正規化されていることが条件。最後に誤差を平均化するため配列数で割る。（これにより、それぞれのバッチのパーセプトロンの数（＝配列数）や次元数（＝配列の長さ）を変動させても互いに損失関数として比較可能とすることが出来る。）　→　膨大なデータの中から任意の数のデータのみをサンプリングして機械学習に流し込むことができるので、計算量を削減できる。しかも、サンプリングパーセンテージはその配列毎に変えることができる。（batch_size）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTデータを使ってミニバッチ学習を実践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MNISTデータのロード\n",
    "import sys,os\n",
    "sys.path.append(os.pardir) #mnist.pyを呼び出すための設定\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#flattenで行列ではなく配列にする\n",
    "#normalizeで0〜1に正規化する\n",
    "(x_train, t_train),  (x_test, t_test) = load_mnist(normalize=False, one_hot_label=True)\n",
    "#1画素 = 0〜256\n",
    "\n",
    "print(x_train.shape) # 784画素 x 6万件の学習データ\n",
    "print(t_train.shape) # 6万件の学習ラベルデータ\n",
    "print(x_test.shape) # 784画素 x 1万件のテストデータ\n",
    "print(t_test.shape) # 1万件のテストラベルデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MNISTデータからランダムサンプリング\n",
    "batch_size = 10\n",
    "np.random.choice(x_train.shape[0], batch_size)#0～60000の配列のうち、どの配列を使うか選んで数字でインデックスを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ミニバッチ対応版（交差エントロピー誤差　損失関数の実装）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    # One-Hot表現の場合、0 or 1なので、\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 認識精度よりも損失度を認識度の指標とする理由"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 認識精度はどのレベルまで認識できれば良いのか上限が見えず、たまたま100%になる（微分し傾きが0になる場所が多く存在する）ことがありデータ表現として0～1の間で表せられないことが多い。（機械が扱うには少し弱いデータ）\n",
    "- 誤差精度は滅多に100%になることがなく、ゆるやかに収束する（微分し傾きが0になる場所が滅多に存在しない）ため微細なパラメータ調整の結果得られる誤差移動率（＝微分した際の傾き）が算出可能となり0～1の間で表せられる。（機械が扱いやすい）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#plt.show()をinlineにて表示する\n",
    "#Esc+Lで行番号を表示する1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "d\n",
    "x1 = np.arange(-10,10,0.001) #-0 to 10 by 0.001\n",
    "y1 = 1.0/(1.0 + np.e ** (-1.0*x))\n",
    "\n",
    "x2 = x1\n",
    "y2 = x2*0\n",
    "\n",
    "x3 = x1\n",
    "y3 = x3*0+1\n",
    "\n",
    "plt.plot(x1,y1,label=\"1/(1+e**-x)\", linestyle=\"-\")\n",
    "plt.plot(x2,y2,label=\"0\", linestyle=\"-\")\n",
    "plt.plot(x3,y3,label=\"1\", linestyle=\"-\")\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.title(\"Standard Sigmoid function#Gain=1 has no gradients=ZERO\")\n",
    "\n",
    "plt.legend()#show label box\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差の遷移に傾き（微分）を使う際の丸め誤差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.float32(1e-50)\n",
    "np.flo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微分方程式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f'(x)=\\frac{d}{dx}f(x)=\\lim_{\\Delta x \\to 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数値微分プログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    \"\"\"Bad Practice\"\"\"\n",
    "    h = 10e-4\n",
    "    return (f(x+h) - f(x)) / 2*h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 解析的な微分と数値微分\n",
    "#### 数値微分は誤差が大きい、解析的微分は誤差が小さい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析的微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 通常の微分方程式により定数計算をしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x \n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_diff(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "\n",
    "tf = tangent_line(function_1, 5)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x, y2)\n",
    "plt.show()\n",
    "#傾きが0.2になっていない！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#略（微分が対象軸（＝変数）ごとに実施できることと、複数の微分演算子が微分の順番を変えても最終的には成り立つということが示されていること）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 行列式において、全ての値についての偏微分を求めてベクトル行列化したものを勾配と呼ぶ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    \"\"\"f関数におけるx配列の数値微分をし、それぞれの値の勾配（微分した際の傾きを返します）をxと同じ列数の配列で返します。\"\"\"\n",
    "    h = 1e-4 # 0.0001（微動）\n",
    "    grad = np.zeros_like(x) # 0 * xと同義\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        \n",
    "        #一旦退避。\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        #もしプラスの方向に微動した場合の結果は？\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        #もしマイナスの方向に微動した場合の結果は？\n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        \n",
    "        #配列を数値微分\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        #傾きの計算が終わったので、一旦値を戻す\n",
    "        x[idx] = tmp_val\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配降下のイメージ図（2次元関数の場合）→傾きが0に収束するためのベクトルがプロットされる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)\n",
    "\n",
    "\n",
    "def tangent_line(f, x):\n",
    "    d = numerical_gradient(f, x)\n",
    "    print(d)\n",
    "    y = f(x) - d*x\n",
    "    return lambda t: d*t + y\n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    x0 = np.arange(-2, 2.5, 0.25)\n",
    "    x1 = np.arange(-2, 2.5, 0.25)\n",
    "    X, Y = np.meshgrid(x0, x1)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    \n",
    "    grad = numerical_gradient(function_2, np.array([X, Y]) )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    plt.xlim([-2, 2])\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数が収束する方向にベクトルが向いていることに注意。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "関数の傾きが0となる点を目指して関数の値を徐々に収束させる手法。\n",
    "今回の場合は損失関数を0に収束させるため、勾配降下法と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_0 = x_0 - \\eta(\\partial f/\\partial x_0) , x_1 = x_1 - \\eta(\\partial f/\\partial x_1) $$\n",
    "$$ \\eta = 学習率 (概ね、0.01や0.001などの値になる。ハイパーパラメータ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配法プログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from gradient_2d import numerical_gradient\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    \"\"\"f : 最適化が必要な関数, init_xは初期値, lrは学習率, step_numは勾配降下回数. fの傾きが最小となるような\"\"\"\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x,np.array(x_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x_0, x_1) = x_0^2 + x_1^2 $$ の最小値を勾配法プログラムによって求める。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])    \n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "偏微分の公式おさらい\n",
    "$$\n",
    "\\displaystyle{ \\left( k f_{x}\\right) = k f_{x} } \\\\\n",
    "\\displaystyle{ \\left(  f \\pm g \\right)_{x} = f_{x} \\pm g_{x} } \\\\\n",
    "\\displaystyle{ \\left(  f \\cdot g \\right)_{x} = f_{x} \\cdot g + f \\cdot g_{x} } \\\\\n",
    "\\displaystyle{ \\left(  \\frac{f}{g} \\right)_{x} = \\frac{ f_{x} \\cdot g – f \\cdot g_{x} }{g^2} } \\\\\n",
    "\\begin{aligned}\n",
    "      \\frac{\\partial f}{\\partial x} &= \\frac{d f}{du} \\frac{\\partial u}{\\partial x} \\\\\n",
    "      \\frac{\\partial f}{\\partial y} &= \\frac{d f}{du} \\frac{\\partial u}{\\partial y}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワーク（行列式）における勾配降下プログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)#2行x3列の行列として作成\n",
    "        #ガウス分布で初期化\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"内積をとる（自身のW（重み）を積算）\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"x = 入力データ、 t = テストデータ\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)#0〜1の値へと汎化\n",
    "        loss = cross_entropy_error(y, t) #tはone-shot行列とする（正解=1、不正解=0）\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])# 1行2列\n",
    "t = np.array([0, 0, 1]) #1行3列\n",
    "\n",
    "net = simpleNet()\n",
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(\"W=\",net.W)\n",
    "print(\"dW=\",dW)\n",
    "print(\"WをdWを下げるようにベクトルを向ける必要がある\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 学習アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ミニバッチ（訓練データサンプリング）\n",
    "    - 訓練データの中からランダムに一部のデータを選びだす。その選ばれたデータをミニバッチと言い、ここでは、そのミニバッチの損失関数の値を減らすことを目的とする。（ミニバッチは無作為サンプリングの手法のため、__確率的勾配降下法(stochastic gradient descent, SGD)__と呼ばれる）\n",
    "2. 勾配算出\n",
    "    - ミニバッチの損失関数を減らすために、各重みのパラメータの勾配を求める。勾配は損失関数の値をもっとも減らす方向を示す。\n",
    "3. パラメータの更新\n",
    "    - 重みパラメータを勾配方向に微小量だけ更新する。\n",
    "4. 繰り返し\n",
    "    - 1 ~ 3 の繰り返し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \"\"\"\n",
    "    2層ニューラルネットワークの実装\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        \"\"\"input_size = 入力の数(MNISTの場合、28 x 28の画素=784), hidden_size = 隠れ層の数(ハイパーパラメータ), output_size = 出力の数(MNISTの場合、0〜9の数字=10), weight_init_std = 重みの標準化引数(?)\"\"\"\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size) #ガウス分布による重みパラメータの初期化\n",
    "        self.params['b1'] = np.zeros(hidden_size) #一様分布によるバイアスパラメータの初期化\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) #ガウス分布による重みパラメータの初期化\n",
    "        self.params['b2'] = np.zeros(output_size) #一様分布によるバイアスパラメータの初期化\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"入力に対する出力を確率配列として返す\"\"\"\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"誤差を交差エントロピー誤差関数によって返す（y =確率配列 , t =one-hot配列）\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"正確度を返す。x = 入力データ、t = テストデータ、accuracy = **%\"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2層ニューラルネットワークを使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__学習データにおける正確さとテストデータにおける正確さが概ね合致しているので、「過学習」は起きていないと思われる！__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
